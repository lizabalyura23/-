{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1eh2H8oeqGe5nOLk0MvAN21ZCgOHguULm",
      "authorship_tag": "ABX9TyOFV+6K8ZOUraBdnpJ+murM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizabalyura23/-/blob/main/model%2Bbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3qCBR84xEv_",
        "outputId": "93f9b206-6d45-4a04-a525-c1a6c36b91af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3135 files belonging to 36 classes.\n",
            "Found 359 files belonging to 36 classes.\n",
            "Found 351 files belonging to 36 classes.\n",
            "Found 3135 images belonging to 36 classes.\n",
            "Found 351 images belonging to 36 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n",
            "Found 359 images belonging to 36 classes.\n",
            "Epoch 1/20\n",
            " 6/98 [>.............................] - ETA: 13:26 - loss: 3.6028 - accuracy: 0.0625"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "98/98 [==============================] - ETA: 0s - loss: 1.8582 - accuracy: 0.5072 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r98/98 [==============================] - 1109s 11s/step - loss: 1.8582 - accuracy: 0.5072 - val_loss: 0.6189 - val_accuracy: 0.8405\n",
            "Epoch 2/20\n",
            "98/98 [==============================] - 388s 4s/step - loss: 0.8034 - accuracy: 0.7528 - val_loss: 0.4715 - val_accuracy: 0.8291\n",
            "Epoch 3/20\n",
            "98/98 [==============================] - 335s 3s/step - loss: 0.5902 - accuracy: 0.8045 - val_loss: 0.3808 - val_accuracy: 0.8661\n",
            "Epoch 4/20\n",
            "98/98 [==============================] - 352s 4s/step - loss: 0.4707 - accuracy: 0.8376 - val_loss: 0.3312 - val_accuracy: 0.8860\n",
            "Epoch 5/20\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.4439 - accuracy: 0.8456Epoch 6/20\n",
            "98/98 [==============================] - 350s 4s/step - loss: 0.3512 - accuracy: 0.8759 - val_loss: 0.2587 - val_accuracy: 0.9117\n",
            "Epoch 7/20\n",
            "98/98 [==============================] - 339s 3s/step - loss: 0.3283 - accuracy: 0.8788 - val_loss: 0.2893 - val_accuracy: 0.9117\n",
            "Epoch 8/20\n",
            "98/98 [==============================] - ETA: 0s - loss: 0.2825 - accuracy: 0.8976Restoring model weights from the end of the best epoch: 6.\n",
            "98/98 [==============================] - 331s 3s/step - loss: 0.2825 - accuracy: 0.8976 - val_loss: 0.2694 - val_accuracy: 0.9288\n",
            "Epoch 8: early stopping\n",
            "12/12 [==============================] - 96s 8s/step - loss: 0.2488 - accuracy: 0.9109\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import Callback, ModelCheckpoint,EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.applications import mobilenet_v2\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—É—Ç–∏ –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è–º –¥–ª—è –æ–±—É—á–∞—é—â–µ–≥–æ, –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –∏ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö\n",
        "train_dir = pathlib.Path(\"/content/drive/MyDrive/datasets/train\")\n",
        "test_dir = pathlib.Path(\"/content/drive/MyDrive/datasets/test\")\n",
        "val_dir = pathlib.Path(\"/content/drive/MyDrive/datasets/validation\")\n",
        "\n",
        "\n",
        "# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
        "img_height = 224\n",
        "img_weigth = 224\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é image_dataset_from_directory\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(train_dir)\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(test_dir)\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(val_dir)\n",
        "# –ü–æ–ª—É—á–∞–µ–º –∏–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤\n",
        "class_names = dict(zip(train_ds.class_names, range(len(train_ds.class_names))))\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –∏—Ö –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "train_generator = ImageDataGenerator(\n",
        "preprocessing_function = mobilenet_v2.preprocess_input,\n",
        "rotation_range = 32,\n",
        "zoom_range = 0.2,\n",
        "width_shift_range = 0.2,\n",
        "height_shift_range = 0.2,\n",
        "shear_range = 0.2,\n",
        "horizontal_flip = True,\n",
        "fill_mode = \"nearest\")\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ\n",
        "train = train_generator.flow_from_directory(train_dir,\n",
        "target_size = (img_height,img_weigth),\n",
        "# –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–º–µ–µ—Ç 3 —Ü–≤–µ—Ç–æ–≤—ã—Ö –∫–∞–Ω–∞–ª–∞\n",
        "color_mode = \"rgb\",\n",
        "# —Å–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–∞\n",
        "class_mode = \"categorical\",\n",
        "batch_size = 32,\n",
        "shuffle = True,\n",
        "seed = 123)\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "validation = train_generator.flow_from_directory(val_dir,\n",
        "target_size = (img_height,img_weigth),\n",
        "# –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–º–µ–µ—Ç 3 —Ü–≤–µ—Ç–æ–≤—ã—Ö –∫–∞–Ω–∞–ª–∞\n",
        "color_mode = \"rgb\",\n",
        "# —Å–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–∞\n",
        "class_mode = \"categorical\",\n",
        "batch_size = 32,\n",
        "shuffle = True,\n",
        "seed = 123)\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å MobileNetV2 —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ ImageNet\n",
        "mobilenet_ = MobileNetV2(\n",
        "input_shape = (img_height,img_weigth,3),\n",
        "include_top = False,\n",
        "weights = 'imagenet',\n",
        "pooling = 'avg')\n",
        "\n",
        "# –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º —Å–ª–æ–∏ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏\n",
        "mobilenet_.trainable = False\n",
        "\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å, –¥–æ–±–∞–≤–∏–≤ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ —Å–ª–æ–∏ –ø–æ–≤–µ—Ä—Ö –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏\n",
        "inputs = mobilenet_.input\n",
        "x = Dense(128, activation = 'relu')(mobilenet_.output)\n",
        "x = Dense(128, activation = 'relu')(x)\n",
        "outputs = Dense(num_classes , activation = 'softmax')(x)\n",
        "\n",
        "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–ª–Ω—É—é –º–æ–¥–µ–ª—å\n",
        "mobilenet = Model(inputs = inputs, outputs = outputs)\n",
        "\n",
        "# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–∞–Ω–Ω—é—é –æ—Å—Ç–∞–Ω–æ–≤–∫—É –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏\n",
        "early_stopping = EarlyStopping(\n",
        "\tmonitor='val_loss',\n",
        "\tmode='min',\n",
        "\tpatience = 2,\n",
        "\tverbose=1,\n",
        "\trestore_best_weights=True,\n",
        ")\n",
        "checkpoint =ModelCheckpoint('/content/drive/MyDrive/fruit224mobile.h5',\n",
        "                        \tmonitor = 'val_loss',\n",
        "                        \tmode = 'min',\n",
        "                       \tsave_best_only = True)\n",
        "\n",
        "# –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º Adam –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏–µ–π –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
        "callbacks = [early_stopping, checkpoint]\n",
        "\n",
        "mobilenet.compile(optimizer=Adam(learning_rate=0.001), loss ='categorical_crossentropy',metrics = ['accuracy'])\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "test = train_generator.flow_from_directory(test_dir,\n",
        "target_size = (224,224),\n",
        "color_mode = \"rgb\",\n",
        "class_mode = \"categorical\",\n",
        "batch_size = 32,\n",
        "shuffle = False)\n",
        "\n",
        "# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–≤–æ–¥–∏–º –≤–∞–ª–∏–¥–∞—Ü–∏—é –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "history = mobilenet.fit(\n",
        "train, validation_data = validation,\n",
        "epochs = 20,\n",
        "callbacks = callbacks)\n",
        "\n",
        "# –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "(eval_loss, eval_accuracy) = mobilenet.evaluate(test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
        "model_path = '/content/drive/MyDrive/fruit224mobile.h5'\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# –ú–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤\n",
        "class_names = {\n",
        "    0: '–Ø–±–ª–æ–∫–æ',\n",
        "    1: '–ë–∞–Ω–∞–Ω',\n",
        "    2: '–°–≤–µ–∫–ª–∞',\n",
        "    3: '–ë–æ–ª–≥–∞—Ä—Å–∫–∏–π –ø–µ—Ä–µ—Ü',\n",
        "    4: '–ö–∞–ø—É—Å—Ç–∞',\n",
        "    5: '–°—Ç—Ä—É—á–∫–æ–≤—ã–π –ø–µ—Ä–µ—Ü',\n",
        "    6: '–ú–æ—Ä–∫–æ–≤—å',\n",
        "    7: '–¶–≤–µ—Ç–Ω–∞—è –∫–∞–ø—É—Å—Ç–∞',\n",
        "    8: '–ü–µ—Ä–µ—Ü —á–∏–ª–∏',\n",
        "    9: '–ö—É–∫—É—Ä—É–∑–∞',\n",
        "    10: '–û–≥—É—Ä–µ—Ü',\n",
        "    11: '–ë–∞–∫–ª–∞–∂–∞–Ω',\n",
        "    12: '–ß–µ—Å–Ω–æ–∫',\n",
        "    13: '–ò–º–±–∏—Ä—å',\n",
        "    14: '–í–∏–Ω–æ–≥—Ä–∞–¥',\n",
        "    15: '–•–∞–ª–∞–ø–µ–Ω—å–æ',\n",
        "    16: '–ö–∏–≤–∏',\n",
        "    17: '–õ–∏–º–æ–Ω',\n",
        "    18: '–õ–∞—Ç—É–∫',\n",
        "    19: '–ú–∞–Ω–≥–æ',\n",
        "    20: '–õ—É–∫',\n",
        "    21: '–ê–ø–µ–ª—å—Å–∏–Ω',\n",
        "    22: '–ü–∞–ø—Ä–∏–∫–∞',\n",
        "    23: '–ì—Ä—É—à–∞',\n",
        "    24: '–ì–æ—Ä–æ—Ö',\n",
        "    25: '–ê–Ω–∞–Ω–∞—Å',\n",
        "    26: '–ì—Ä–∞–Ω–∞—Ç',\n",
        "    27: '–ö–∞—Ä—Ç–æ—Ñ–µ–ª—å',\n",
        "    28: '–†–µ–¥—å–∫–∞',\n",
        "    29: '–°–æ–µ–≤—ã–µ –±–æ–±—ã',\n",
        "    30: '–®–ø–∏–Ω–∞—Ç',\n",
        "    31: '–°–ª–∞–¥–∫–∞—è –∫—É–∫—É—Ä—É–∑–∞',\n",
        "    32: '–ë–∞—Ç–∞—Ç',\n",
        "    33: '–ü–æ–º–∏–¥–æ—Ä',\n",
        "    34: '–†–µ–ø–∞',\n",
        "    35: '–ê—Ä–±—É–∑'\n",
        "}\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–ª–∞—Å—Å–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "def predict_image(image_path):\n",
        "    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
        "\n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞\n",
        "    predictions = model.predict(img)\n",
        "    predicted_class = np.argmax(predictions[0])\n",
        "    predicted_label = class_names[predicted_class]\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "image_path = '/content/Image_3.jpg'  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –ø—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é\n",
        "predicted_label = predict_image(image_path)\n",
        "print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {predicted_label}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdiS1lHOvscJ",
        "outputId": "f773dff9-082d-4c0a-b8af-48db3d82d7cc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 843ms/step\n",
            "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å: –ë–∞–Ω–∞–Ω\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aiogram"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1cRPI_sTCBT",
        "outputId": "e666566d-c7b9-42a6-fb32-5dd04dc3d15b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aiogram\n",
            "  Downloading aiogram-3.6.0-py3-none-any.whl (540 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/540.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m112.6/540.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m \u001b[32m532.5/540.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m540.1/540.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles~=23.2.1 (from aiogram)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: aiohttp~=3.9.0 in /usr/local/lib/python3.10/dist-packages (from aiogram) (3.9.5)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from aiogram) (2024.2.2)\n",
            "Collecting magic-filter<1.1,>=1.0.12 (from aiogram)\n",
            "  Downloading magic_filter-1.0.12-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pydantic<2.8,>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from aiogram) (2.7.1)\n",
            "Requirement already satisfied: typing-extensions<=5.0,>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from aiogram) (4.11.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (4.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.8,>=2.4.1->aiogram) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.8,>=2.4.1->aiogram) (2.18.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp~=3.9.0->aiogram) (3.7)\n",
            "Installing collected packages: magic-filter, aiofiles, aiogram\n",
            "Successfully installed aiofiles-23.2.1 aiogram-3.6.0 magic-filter-1.0.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#–ü–û–ë–ï–î–ê!!!!!!\n",
        "import logging\n",
        "import os\n",
        "from aiogram import Bot, Dispatcher, types\n",
        "from aiogram.filters.command import Command\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from aiogram import F\n",
        "import asyncio\n",
        "import cv2\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –±–æ—Ç–∞ —Å —Ç–æ–∫–µ–Ω–æ–º\n",
        "bot = Bot(token='6959067623:AAFphw5BAC0dqooMfve_TCFnwXlz-m8eEr4')\n",
        "dp = Dispatcher()\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–ª–∞—Å—Å–∞\n",
        "def predictions(img_path):\n",
        "    classifier = tf.keras.models.load_model(\"/content/drive/MyDrive/fruit224mobile.h5\")\n",
        "    class_labels = {\n",
        "        0: '–Ø–±–ª–æ–∫–æ',\n",
        "        1: '–ë–∞–Ω–∞–Ω',\n",
        "        2: '–°–≤–µ–∫–ª–∞',\n",
        "        3: '–ë–æ–ª–≥–∞—Ä—Å–∫–∏–π –ø–µ—Ä–µ—Ü',\n",
        "        4: '–ö–∞–ø—É—Å—Ç–∞',\n",
        "        5: '–°—Ç—Ä—É—á–∫–æ–≤—ã–π –ø–µ—Ä–µ—Ü',\n",
        "        6: '–ú–æ—Ä–∫–æ–≤—å',\n",
        "        7: '–¶–≤–µ—Ç–Ω–∞—è –∫–∞–ø—É—Å—Ç–∞',\n",
        "        8: '–ü–µ—Ä–µ—Ü —á–∏–ª–∏',\n",
        "        9: '–ö—É–∫—É—Ä—É–∑–∞',\n",
        "        10: '–û–≥—É—Ä–µ—Ü',\n",
        "        11: '–ë–∞–∫–ª–∞–∂–∞–Ω',\n",
        "        12: '–ß–µ—Å–Ω–æ–∫',\n",
        "        13: '–ò–º–±–∏—Ä—å',\n",
        "        14: '–í–∏–Ω–æ–≥—Ä–∞–¥',\n",
        "        15: '–•–∞–ª–∞–ø–µ–Ω—å–æ',\n",
        "        16: '–ö–∏–≤–∏',\n",
        "        17: '–õ–∏–º–æ–Ω',\n",
        "        18: '–õ–∞—Ç—É–∫',\n",
        "        19: '–ú–∞–Ω–≥–æ',\n",
        "        20: '–õ—É–∫',\n",
        "        21: '–ê–ø–µ–ª—å—Å–∏–Ω',\n",
        "        22: '–ü–∞–ø—Ä–∏–∫–∞',\n",
        "        23: '–ì—Ä—É—à–∞',\n",
        "        24: '–ì–æ—Ä–æ—Ö',\n",
        "        25: '–ê–Ω–∞–Ω–∞—Å',\n",
        "        26: '–ì—Ä–∞–Ω–∞—Ç',\n",
        "        27: '–ö–∞—Ä—Ç–æ—Ñ–µ–ª—å',\n",
        "        28: '–†–µ–¥—å–∫–∞',\n",
        "        29: '–°–æ–µ–≤—ã–µ –±–æ–±—ã',\n",
        "        30: '–®–ø–∏–Ω–∞—Ç',\n",
        "        31: '–°–ª–∞–¥–∫–∞—è –∫—É–∫—É—Ä—É–∑–∞',\n",
        "        32: '–ë–∞—Ç–∞—Ç',\n",
        "        33: '–ü–æ–º–∏–¥–æ—Ä',\n",
        "        34: '–†–µ–ø–∞',\n",
        "        35: '–ê—Ä–±—É–∑'\n",
        "    }\n",
        "    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "    img = cv2.imread(img_path)\n",
        "    # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ BGR –≤ RGB\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–∑–º–µ—Ä–µ–Ω–∏—è –¥–ª—è batch\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "    img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
        "\n",
        "\n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞\n",
        "    predictions = classifier.predict(img)\n",
        "    predicted_class = np.argmax(predictions[0])\n",
        "    predicted_label = class_labels[predicted_class]\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –µ–≥–æ –≤ –º–∞—Å—Å–∏–≤\n",
        "def get_img_array(img_path, size):\n",
        "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n",
        "    array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start\n",
        "@dp.message(Command(\"start\"))\n",
        "async def start(message: types.Message):\n",
        "    await message.reply('–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç, —Ä–∞—Å–ø–æ–∑–Ω–∞—é—â–∏–π –æ–≤–æ—â–∏ –∏ —Ñ—Ä—É–∫—Ç—ã')\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /help\n",
        "@dp.message(Command(\"help\"))\n",
        "async def help(message: types.Message):\n",
        "    await message.reply('–ü—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–≤–æ—â –∏–ª–∏ —Ñ—Ä—É–∫—Ç')\n",
        "\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π\n",
        "@dp.message(F.photo)\n",
        "async def download_photo(message: types.Message, bot: Bot):\n",
        "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª\n",
        "    img_path = f\"tmp/{message.photo[-1].file_id}.jpg\"\n",
        "    await bot.download(\n",
        "        message.photo[-1],\n",
        "        destination=img_path\n",
        "    )\n",
        "    print(f\"Downloaded photo path: {img_path}\")\n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –æ–±—ä–µ–∫—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏\n",
        "    pred = predictions(img_path)\n",
        "    await message.answer(f\"–Ø –¥—É–º–∞—é, —á—Ç–æ —ç—Ç–æ {pred} üòä\")\n",
        "\n",
        "\n",
        "#–∑–∞–ø—É—Å–∫ –±–æ—Ç–∞\n",
        "async def main():\n",
        "    await dp.start_polling(bot)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "6L3NNVXMlcac",
        "outputId": "26d56cd6-683c-47cc-bca3-ea041696834b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "asyncio.run() cannot be called from a running event loop",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e4eb6b8ce2e0>\u001b[0m in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m     34\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#–ø–æ–±–µ–¥–∞\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhSdFyTamZl4",
        "outputId": "9d881b0b-a743-45be-befa-3306a57972af"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded photo path: tmp/AgACAgIAAxkBAAMxZk9C6AM90t05EezpZDEzY8J_8qwAAlHkMRtz7IBKzNWOM34TT6oBAAMCAAN5AAM1BA.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7e9d48a29630> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 856ms/step\n",
            "Downloaded photo path: tmp/AgACAgIAAxkBAAM0Zk9DGidmSW6cLkb73c5UfT6PJTQAAoLVMRt-WnFK2WsOHOYpktwBAAMCAAN4AAM1BA.jpg\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Downloaded photo path: tmp/AgACAgIAAxkBAAM2Zk9DIJR6Ff4qupArI0MCx5IjZo4AAlPkMRtz7IBKpFeE4BF5_4IBAAMCAAN5AAM1BA.jpg\n",
            "1/1 [==============================] - 1s 858ms/step\n",
            "Downloaded photo path: tmp/AgACAgIAAxkBAAM4Zk9D0c882A63iYtFXnWukGINWrIAAn7dMRsSOXhKyh8myYERx9kBAAMCAAN5AAM1BA.jpg\n",
            "1/1 [==============================] - 1s 876ms/step\n",
            "Downloaded photo path: tmp/AgACAgIAAxkBAAM6Zk9D8Vr88UaPFYZz16WBAvaGAU4AAn_dMRsSOXhKMFfPc_-A9BcBAAMCAAN5AAM1BA.jpg\n",
            "1/1 [==============================] - 1s 860ms/step\n",
            "Downloaded photo path: tmp/AgACAgIAAxkBAAM8Zk9EDMkeBmrtvPTm7EPsPFT4bAIAAoHdMRsSOXhK9wXG-t6ferABAAMCAAN5AAM1BA.jpg\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "Downloaded photo path: tmp/AgACAgIAAxkBAAM-Zk9ERKhiK0lfE6t1iayEtVeE9pIAAoLdMRsSOXhKn_MilyZu3eQBAAMCAAN5AAM1BA.jpg\n",
            "1/1 [==============================] - 1s 817ms/step\n",
            "Downloaded photo path: tmp/AgACAgIAAxkBAANAZk9EbuU1HU5cN8rSlW_HfHyq9NEAAojdMRsSOXhKpGIKWPvXzEABAAMCAAN5AAM1BA.jpg\n",
            "1/1 [==============================] - 1s 831ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:aiogram.dispatcher:Received SIGINT signal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MTLd4UO72kGJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}